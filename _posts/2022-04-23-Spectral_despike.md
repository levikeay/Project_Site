---
title: Data Cleaning for Optical Satellite Timeseries Containing Cloud
layout: post
usemathjax: true
post-image: "https://user-images.githubusercontent.com/63168148/186032134-c36e72a8-8323-4c52-a469-1d61f7af20f5.gif"
description: 

---

## Context: breakpoint detection in satellite imagery timeseries.
Breakpoint detection in optical satellite imagery timeseries is a promising area of development, especially with the rise of commercial cubesat
imaging platforms such as Planet. In this process, a series of satellite images is acquired over an area of interest, a spectral index is computed, and the area represented by a single pixel is tracked using this index over time. By looking at the resulting timeseries of each pixel, meaningful temporal-spatial insights can be gained. Because of the vast amounts of data being generated by satellite imaging missions, it is important to develop fully automated methods for processing in order to take full advantage of the dataset.

There are multiple processing steps needed before timeseries analysis can be performed, such as calibrating and aligning the images before computing their pixel time-series. Another key step in the processing of optical images is the elimination of clouds from the imagery. This can be done in a variety of methods, but often these methods are imperfect and some cloudy pixels remain in the imagery stack. In the name of full automation of the breakpoint detection pipeline, it is important to have a back up algorithm to deal with persisting cloud in the timeseries.

## Clouds and NDVI
The Normalized Difference Vegetation Index (NDVI) is a popular spectral index which is a metric of plant health. It is applicable to cubesat data because it only requires 2 spectral bands. It is computed using the Near-Infrared (NIR) and Red bands as:

> NDVI = $$ (NIR - Red)/(NIR + Red) $$

Clouds appear **dark** in NDVI, and so a pixel area which normally images vegetation but is obstructed by cloud on one observation will appear as a downwards spike in the timeseries. This is simulated in the animations below:

I have made a simple modification to the prior approach of cloud despiking designed for annual best available pixel (BAP) datasets, which have regularly spaced timeseries data (1 point per year). For this approach it is fitting to take the mean of neighbouring values, as it is equivalent to interpolation when the data is evenly spaced.

However with irregularly spaced observations as is found with high cadence cubesat data, an extra few lines of code are required to calculate the linearly interpolated values of each data points' neighbours

Here is an example of my algorithm in progress, despiking some artificial data, with the large initial spike meant to represent a cloud:


![despike_upperenvelope2](https://user-images.githubusercontent.com/63168148/186097612-0c4c06d0-09df-4e83-b4fe-889eccd94588.gif)


It is important to consider the threshold of despiking. This parameter sets the tolerance of the algorithm. On each iteration, the points of the timeseries are compared to the interpolated values of their neighbours, and the point of largest discrepancy between timeseries value and neighbour-interpolation is despiked only if it is larger than the despiking threshold. 

Setting the threshold too high, you will not manage to despike anything. 

Set the threshold too low, and you will remove valuable information from the timeseries by over-despiking. An example of this is illustrated in the following animation, where the threshold is set to 0.5:

![despike_toomuch](https://user-images.githubusercontent.com/63168148/186097661-cdd028ef-6a51-48c1-bb4b-7bf5b9f4ce85.gif)

